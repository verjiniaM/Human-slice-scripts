{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e59161",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "[Halfwidth manual adjustment](#1) <br>\n",
    "[Capacitance manual adjustment](#2) <br>\n",
    "[Adding a parameter to QC datatables](#3) <br>\n",
    "\n",
    "<br>\n",
    "\n",
    "[Analysis mouse experiments](#4) <br>\n",
    "[Analysis miniML averages output](#5) <br>\n",
    "[Prepare files for analysis with Barbara's algorithm - manual analysis](#6) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b3a92",
   "metadata": {},
   "source": [
    "## Halfwidth manual adjustment <a name= \"1\">\n",
    "    \n",
    " 1st definition can be played around with the TH speed, if the TH was detected before the actual TH.\n",
    "\n",
    "2nd definition the lower and upper bounds can be adjusted, if, for example, the AP is very fast and there are not enough datapoints between .3 and .7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4859c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/code/Human-slice-scripts/detect_peaks.py:11: UserWarning: A newest version is available at https://pypi.org/project/detecta/\n",
      "  warnings.warn('A newest version is available at https://pypi.org/project/detecta/')\n"
     ]
    }
   ],
   "source": [
    "import sorting_functions as sort\n",
    "import human_characterisation_functions as hcf\n",
    "import numpy as np\n",
    "import plotting_funcs\n",
    "import pyabf\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import math\n",
    "from detect_peaks import detect_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9d5967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ap_param(charact_data, channels, inj, max_spikes, peak_loc_x = 80, th_speed = 10):\n",
    "    '''\n",
    "    returns a nd.array with inj, peak loc, sweep numx\n",
    "    spike_counts for each inj\n",
    "    array with all peak locations [inj, peak, sweep num]\n",
    "    peak_loc_x defines how wide the AP is defined around the peak, how many datapoints,\n",
    "    smaller values certainly catch only 1 AP\n",
    "    '''\n",
    "    params = [Rheobase_all, AP_all, THloc_all, TH_all, APheight_all, max_depol_all, max_repol_all] = [[], [], [], [], [], [], []]\n",
    "    for a, ch in enumerate(channels):\n",
    "        key = 'Ch' + str(ch)\n",
    "        ch1 = charact_data[key][0]\n",
    "        \n",
    "        peaks = np.empty([len(inj), max_spikes[a], 3])\n",
    "        peaks.fill(np.nan)\n",
    "        for i in range(len(ch1[0])): #for all swps\n",
    "            pks = detect_peaks(ch1[:,i] , mph=2,mpd=10)  #mph=20,mpd=50\n",
    "            peaks[i,0:len(pks), 0] = inj[i] #injected current\n",
    "            peaks[i,0:len(pks), 1] = pks #\n",
    "            peaks[i,0:len(pks), 2] = ch1[pks,i] #sweep number\n",
    "        # number of spikes in each step\n",
    "        spike_counts = np.ndarray([len(inj),2])\n",
    "        for i, j in enumerate(inj):\n",
    "            spike_counts[i,0] = j\n",
    "            spike_counts[i,1] = (np.sum(np.isfinite(peaks[i,:,:])))/3\n",
    "        \n",
    "        spikes = np.where(np.isfinite(peaks)) \n",
    "\n",
    "        first_spike = spikes[0][0]\n",
    "        Rheobase = inj[first_spike]  \n",
    "        \n",
    "        if np.max(spike_counts[:,1]) == 1:\n",
    "            print('MAX number of AP = 1 for ' + key)\n",
    "            first_spiking_sweep = np.where(spike_counts[:,1]==1)[0][0]\n",
    "        else:\n",
    "            first_spiking_sweep = np.where(spike_counts[:,1]>1)[0][0] #where there is more than 1 AP\n",
    "        \n",
    "        if np.max(spike_counts[:,1]) == 1:\n",
    "            ap = 0\n",
    "        else:\n",
    "            ap = 1\n",
    "\n",
    "        peak_loc = np.where(ch1[:,first_spiking_sweep] == peaks[first_spiking_sweep, ap ,2])[0][0]\n",
    "        AP = ch1[:, first_spiking_sweep][peak_loc - peak_loc_x:peak_loc + peak_loc_x]\n",
    "        d1_AP = np.diff(AP)*20\n",
    "        THlocs = np.where(d1_AP[:peak_loc_x] > th_speed)\n",
    "        if THlocs[0].size != 0:\n",
    "            THloc = THlocs[0][0]\n",
    "            TH = AP[THloc-1]\n",
    "            APheight = peaks[first_spiking_sweep, ap ,2]-TH #amplitude of AP, the 2nd AP\n",
    "            max_depol = np.max(d1_AP[:peak_loc_x]) #how quickly is the voltage change occuring\n",
    "            max_repol = np.min(d1_AP[-peak_loc_x:])\n",
    "            ch_params = [Rheobase, AP, THloc, TH, APheight, max_depol, max_repol]\n",
    "\n",
    "            for i, param in enumerate(params):\n",
    "                param.append(ch_params[i])\n",
    "\n",
    "    return Rheobase_all, AP_all, THloc_all, TH_all, APheight_all, max_depol_all, max_repol_all\n",
    "\n",
    "def get_AP_HW(channels, AP_all, APheight_all, TH_all, lower_bound = 0.1, upper_bound = 0.9):\n",
    "    '''\n",
    "    returns AP halfwidt in ms at 1/2 of the AP amplitude,\n",
    "    measured from AP threshold to AP peak\n",
    "    '''\n",
    "    AP_HWs = []\n",
    "    for i, ch in enumerate(channels):\n",
    "        if len(AP_all) == 0:\n",
    "            AP_HWs.append(math.nan)\n",
    "            continue\n",
    "\n",
    "        half_amp = TH_all[i] + APheight_all[i]/2\n",
    "\n",
    "        t30 = TH_all[i] + APheight_all[i] * lower_bound\n",
    "        t70 = TH_all[i] + APheight_all[i] * upper_bound\n",
    "\n",
    "        rise_start = np.where(AP_all[i] > t30)[0][0]\n",
    "        rise_end = np.where(AP_all[i] > t70)[0][0]\n",
    "\n",
    "        x = range(rise_start, rise_end)\n",
    "        y = AP_all[i][rise_start:rise_end]\n",
    "         \n",
    "        model_rise = np.polyfit(x, y, 1)\n",
    "        hw1 = (half_amp - model_rise[1])/model_rise[0] \n",
    "\n",
    "        fall_start = np.where(AP_all[i] > t30)[0][-1]\n",
    "        fall_end = np.where(AP_all[i] > t70)[0][-1]\n",
    "\n",
    "        x_fall = range(fall_end, fall_start)\n",
    "        y_fall = AP_all[i][fall_end:fall_start]\n",
    "\n",
    "        model_fall = np.polyfit(x_fall, y_fall, 1)\n",
    "        hw2 = (half_amp - model_fall[1])/model_fall[0]\n",
    "\n",
    "        AP_HWs.append((hw2-hw1)/20) #conversion to ms\n",
    "\n",
    "        predict_rise = np.poly1d(model_rise)\n",
    "        rise_prediction = predict_rise(np.linspace(rise_start, rise_end, 30))\n",
    "        difference_array = np.absolute(np.linspace(rise_start, rise_end, 30)-hw1)\n",
    "        index_rise = difference_array.argmin()\n",
    "        hw1_y = rise_prediction[index_rise]\n",
    "\n",
    "        predict_fall = np.poly1d(model_fall)\n",
    "        fall_prediction = predict_fall(np.linspace(fall_end, fall_start, 30))\n",
    "        difference_array = np.absolute(np.linspace(fall_end, fall_start, 30)-hw2)\n",
    "        index_fall = difference_array.argmin()\n",
    "        hw2_y = fall_prediction[index_fall]\n",
    "\n",
    "    return AP_HWs, rise_start, rise_end, rise_prediction, fall_end, fall_start, fall_prediction, hw1, hw2, hw1_y, hw2_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ad0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AP HALFWIDTH - plot for QC check dataframes\n",
    "#line 53 the peak location might need changing\n",
    "\n",
    "human_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/'\n",
    "#intr_props_dirs = glob.glob(human_dir + '/data_*/' + 'OP*' + '/data_tables/' + 'QC_passed' + '*.xlsx')\n",
    "intr_props_dirs = ['/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/data_verji/OP230417/data_tables/AP_props_OP230417.xlsx']\n",
    "\n",
    "HWs = []\n",
    "for path in intr_props_dirs:\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    for i, fn in enumerate(df['filename']):\n",
    "\n",
    "        file_path = path[:path.rfind('data_tables')] + fn \n",
    "        end_fn = file_path.rfind('/') + 1\n",
    "        dir_ap_HW = sort.make_dir_if_not_existing(file_path[:end_fn] + 'plots/',  'AP_HW')\n",
    "        channels = [int(df.cell_ch[i])]\n",
    "        charact_data = hcf.load_traces(file_path)\n",
    "        inj = hcf.get_inj_current_steps(file_path)\n",
    "        max_spikes = hcf.get_max_spikes(charact_data, channels)\n",
    "        if df['max_spikes'][i] == 0:\n",
    "            HWs.append(math.nan)\n",
    "            continue\n",
    "        first_spikes, peaks_all, spike_counts_all, first_spiking_sweeps_all = hcf.get_ap_param_for_plotting(charact_data, channels, inj, max_spikes)\n",
    "        \n",
    "        #using the adjusted version of the functions, above\n",
    "        Rheobase_all, AP_all, THloc_all, TH_all, APheight_all, max_depol_all, max_repol_all = get_ap_param(charact_data, channels, inj, max_spikes)\n",
    "        AP_HWs, rise_start, rise_end, rise_prediction, fall_end, fall_start, fall_prediction, hw1, hw2, hw1_y, hw2_y = get_AP_HW(channels, AP_all, APheight_all, TH_all)\n",
    "        \n",
    "        if type(rise_start) is list:\n",
    "            continue\n",
    "        key = 'Ch' + str(channels[0])\n",
    "        ch_data = charact_data[key][0]\n",
    "\n",
    "        if math.isnan(first_spikes[0]):\n",
    "            # x = 1\n",
    "            # fig, ax = plt.subplots(x,x,sharex=True, sharey=False,figsize=(10,4))\n",
    "            # fig.suptitle(key, fontsize=15)\n",
    "            # plt.show()\n",
    "            # plt.savefig(dir_ap_props + '/' + filename[end_fn:-4] + '_AP#' + str(0) + '_' + key + '.png')\n",
    "            # plt.close()\n",
    "            continue\n",
    "\n",
    "        if np.max(spike_counts_all[0][:,1]) == 1:\n",
    "            ap = 0\n",
    "        else:\n",
    "            ap = 1\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(AP_all[0], 'o', zorder = 0, label = 'AP trace')\n",
    "        ax.scatter(THloc_all[0],TH_all[0], color = 'orange')\n",
    "         \n",
    "        ax.scatter(175,peaks_all[0][first_spiking_sweeps_all[0], ap, 2], color = 'green')\n",
    "        ax.scatter([hw1, hw2], [hw1_y, hw2_y], c = 'red', zorder = 10, label = 'AP halfwidth')\n",
    "        ax.scatter([rise_start, rise_end, fall_start, fall_end],\\\n",
    "            [AP_all[0][rise_start], AP_all[0][rise_end], AP_all[0][fall_start], AP_all[0][fall_end]],\\\n",
    "                c = 'lightgrey', zorder = 11)\n",
    "        ax.plot(np.linspace(rise_start, rise_end, 30), rise_prediction, c = 'pink', label = 'prediction')\n",
    "        ax.plot(np.linspace(fall_end, fall_start, 30), fall_prediction, c ='lightgreen', label = 'prediction')\n",
    "\n",
    "        ax.text(THloc_all[0] - 60,TH_all[0], 'Threshold')\n",
    "        \n",
    "        #d\n",
    "        ax.text(185,peaks_all[0][first_spiking_sweeps_all[0], ap, 2], 'Peak')\n",
    "\n",
    "        ax.legend()\n",
    "        fig.suptitle('Ch: ' + key + ', AP#' + str(ap+1) + ', TH = ' + str(round(TH_all[0],2)) + ', amp = ' + str(round(APheight_all[0],2)))\n",
    "        fig.patch.set_facecolor('white') \n",
    "        plt.savefig(dir_ap_HW + '/' + file_path[end_fn:-4] + '_HW_AP#' + str(ap+1) + '_' + key + '.png')\n",
    "        #plt.show()\n",
    "        HWs.append(AP_HWs[0])\n",
    "\n",
    "df33 = pd.DataFrame({'AP_HW': HWs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ce658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the parameters and change them in the QC_intrinsic properties dataframe\n",
    "\n",
    "params = [max_spikes, Rheobase_all, APheight_all, TH_all, max_depol_all, max_repol_all, AP_HWs]\n",
    "\n",
    "for param in params:\n",
    "    my_var_name = [ k for k,v in locals().items() if v is param][0]\n",
    "    print(my_var_name, ' = ', str(param[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3960450a",
   "metadata": {},
   "source": [
    "## Capacitance manual adjustemnt  <a name= \"2\">\n",
    "    \n",
    "Sometimes an artifact causes problems\n",
    "To overcome them, we just take the second datapoint after the onset (line 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17006c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/'\n",
    "OP = 'OP231109'\n",
    "patcher = 'Verji'\n",
    "\n",
    "work_dir, filenames, indices_dict, slice_names = sort.get_OP_metadata(human_dir, OP, patcher)\n",
    "\n",
    "fn = work_dir + '23n09053.abf' #caracterization filename\n",
    "inj = hcf.get_inj_current_steps(fn)\n",
    "chans = [2] #\n",
    "\n",
    "print('The inj is', inj[:5])\n",
    "\n",
    "params = tau_all, capacitance_all, mc_all, V65_all = [], [], [], []\n",
    "onset, offset = hcf.find_charact_onset_offset(fn)\n",
    "\n",
    "charact_data = hcf.load_traces(fn)\n",
    "mc = np.ndarray([5,3])\n",
    "for ch in chans:\n",
    "    key = 'Ch' + str(ch)\n",
    "    ch1 = charact_data[key][0]\n",
    "    V65s = []\n",
    "    mc = np.ndarray([5,4])\n",
    "    for i in range(0,5):\n",
    "        I = inj[i]*1e-12 #check step size\n",
    "        bl = np.median(ch1[0:onset-20,i])\n",
    "        ss = np.median(ch1[offset-2000:offset-1,i]) #steady state, during the current step\n",
    "        swp = list(ch1[:,i])\n",
    "        Vdiff = bl-ss #voltage deflection size\n",
    "        v65 = Vdiff * 0.63\n",
    "        V65 = bl - v65\n",
    "        V65s.append(V65)\n",
    "        if list(filter(lambda ii: ii < V65, swp)) == []:\n",
    "            continue\n",
    "        else:\n",
    "            #\n",
    "            res = list(filter(lambda ii: ii < V65, swp))[1] #takes the first value in swp < V65\n",
    "            tau65 = swp.index(res) #index of res\n",
    "            R = (Vdiff/1000)/-I     \n",
    "            tc = tau65 - onset\n",
    "            mc[i,0] = tc * 0.05 #membranec capacitance; tc - time constant\n",
    "            mc[i,1] = R * 1e-6 #resistance\n",
    "            mc[i,2] = tc * 5e-5 / R #capacitance\n",
    "    mc[:,3] = inj[:5]\n",
    "    mc[:,2] = mc[:,2]/1e-12  \n",
    "    tau = mc[1,0]\n",
    "    capacitance = mc[1,2]\n",
    "    ch_params = [tau, capacitance, mc, V65s]\n",
    "\n",
    "    for i, param in enumerate(params):\n",
    "        param.append(ch_params[i])\n",
    "\n",
    "    print(mc, 'ch', str(ch))\n",
    "\n",
    "clrs = [\"b\", \"g\", \"r\", \"c\", \"m\", \"y\", \"#FF4500\", \"#800080\"]\n",
    "\n",
    "end_fn = fn.rfind('/') +1 \n",
    "\n",
    "tau_all, capacitance_all, mcs, V65s = tau_all, capacitance_all, mc_all, V65_all\n",
    "for n, ch in enumerate(chans):\n",
    "    key = 'Ch' + str(ch)\n",
    "    ch_data = charact_data[key][0]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    for i in range(0,5):\n",
    "        swp = list(ch_data[:,i])\n",
    "        bl = np.median(ch_data[0:onset-60,i])\n",
    "        if list(filter(lambda ii: ii < V65s[n][i], swp)) == []:\n",
    "            print(\"No hyperpolarization fig for \" + fn[end_fn:-4] + key)\n",
    "        else:\n",
    "            res = list(filter(lambda ii: ii < V65s[n][i], swp))[1] #takes the first value in swp < V65\n",
    "            tau65 = swp.index(res) #index of res    \n",
    "            tc = tau65 - onset\n",
    "            plt.plot(ch_data[:,i], c = clrs[i])\n",
    "            plt.scatter(onset + tc, V65s[n][i], c=clrs[i])\n",
    "            plt.annotate('V65  ', (onset + tc, V65s[n][i]), horizontalalignment='right')\n",
    "            plt.scatter(onset, bl, c='r')\n",
    "            plt.ylabel('mV')\n",
    "    plt.annotate(' Baseline', (onset, bl))\n",
    "    fig.patch.set_facecolor('white') \n",
    "\n",
    "    dir_onset = work_dir + '/plots/Onset/'\n",
    "    plt.show()\n",
    "    plt.savefig(dir_onset + '/Char_onset_plot_' + fn[end_fn:-4]+'_'+ key + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f760f",
   "metadata": {},
   "source": [
    "## Adding new parameters to the exiscitng QC_checked datatables  <a name= \"3\">\n",
    "    \n",
    "Example, you decide to analyze another parameter and want to include it in all QC data tables. Example code with AP_HWs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import human_characterisation_functions as hcf\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "human_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/'\n",
    "intr_props_dirs = glob.glob(human_dir + '/data_*/' + 'OP*' + '/data_tables/' + 'QC_passed' + '*.xlsx')\n",
    "\n",
    "for path in intr_props_dirs:\n",
    "    df = pd.read_excel(path)\n",
    "\n",
    "    if 'AP_halfwidth' in df.columns:\n",
    "        continue\n",
    "\n",
    "    HWs = []\n",
    "\n",
    "    for i, fn in enumerate(df['filename']):\n",
    "        fn = path[:path.rfind('data_tables')] + fn \n",
    "        chans = [int(df.cell_ch[i])]\n",
    "        charact_data = hcf.load_traces(fn)\n",
    "        inj = hcf.get_inj_current_steps(fn)\n",
    "        max_spikes = hcf.get_max_spikes(charact_data, chans)\n",
    "        Rheobase_all, AP_all, THloc_all, TH_all, APheight_all, max_depol_all, max_repol_all = hcf.get_ap_param(charact_data, chans, inj, max_spikes)\n",
    "        HWs.append(hcf.get_AP_HW(chans, AP_all, APheight_all, TH_all)[0])\n",
    "    df.insert(len(df.columns), 'AP_halfwidth', HWs)\n",
    "    df.to_excel(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cafb6",
   "metadata": {},
   "source": [
    "## Analysis mouse experiments <a name = '4'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca57207e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import sorting_functions as sort\n",
    "import numpy as np\n",
    "import human_characterisation_functions as hcf\n",
    "import plotting_funcs\n",
    "import connection_parameters as con_param\n",
    "import json\n",
    "import math\n",
    "import datetime\n",
    "import shutil\n",
    "import glob\n",
    "import plot_intrinsic_props as plot_intr\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "#this is only for 11.16 because the excel can't be open normally for whatever reason\n",
    "def get_OP_metadata (human_dir, OP, patcher, path_wd):\n",
    "    work_dir = sort.get_work_dir(human_dir, OP, patcher)\n",
    "    file_list = sort.get_sorted_file_list(work_dir)\n",
    "    jsons = sort.get_json_files(file_list)\n",
    "\n",
    "    df_rec = pd.read_excel(path_wd, header = 1)\n",
    "\n",
    "    filenames = sort.get_abf_files(file_list)\n",
    "    slice_indx, def_slice_names, indices_dict = sort.sort_protocol_names(file_list, df_rec)\n",
    "\n",
    "    if OP + '_indices_dict.json' in jsons:\n",
    "        indices_dict = sort.from_json(work_dir, OP, '_indices_dict.json')\n",
    "    else: \n",
    "        sort.to_json(work_dir, OP, '_indices_dict.json', indices_dict)\n",
    "\n",
    "    slice_names = sort.fix_slice_names(def_slice_names, slice_indx)\n",
    "    return work_dir, filenames, indices_dict, slice_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf35c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual inputs\n",
    "human_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/mouse/'\n",
    "\n",
    "OP = '2023.11.16_CA1-sub'\n",
    "tissue_source = ''\n",
    "inj = 'full'\n",
    "age = ''\n",
    "patcher = ''\n",
    "\n",
    "active_chans_meta = [{\"OP_time\": [\"2023-11-16 09:16:00\"],  #13.11 8:44,14.11 08:31, 16.11 - 09:16,  \n",
    "\"slices\": ['S1', 'S2', 'S2_again', 'S3', 'S4'], \n",
    "\"vc_files\": indices_dict['vc'], \n",
    "\"active_chans\": [[7,8], [4,6,7,8], [1,2,4,6,7,8], [7], [4,7,8]]},\n",
    "{\"con_screen_IC_file_indices\": indices_dict['IC_files'], \n",
    "\"post_chans_VC\": [[7,8],[7,8],[7,8],[7,8],[7,8],\n",
    "            [4,6,7,8],[4,6,7,8],[4,6,7,8],[4,6,7,8],[4,6,7,8],\n",
    "            [1,2,4,6,7,8],[1,2,4,6,7,8],[1,2,4,6,7,8],[1,2,4,6,7,8],[1,2,4,6,7,8],[1,2,4,6,7,8],\n",
    "            [7],[4,7,8],[4,7,8]]}]\n",
    "\n",
    "\n",
    "path_wd = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/mouse/2023.11.16_CA1-sub/connectivity_labbook_template_CA1_sub.xlsx'\n",
    "work_dir, filenames, indices_dict, slice_names = get_OP_metadata(human_dir, OP, patcher,path_wd)\n",
    "#for other non-error folders \n",
    "#work_dir, filenames, indices_dict, slice_names = hcf.get_OP_metadata(human_dir, OP, patcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d8a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #creating a dir to save plots and data_tables (if not existing)\n",
    "dir_plots = sort.make_dir_if_not_existing (work_dir, 'plots')\n",
    "sort.make_dir_if_not_existing (work_dir, 'data_tables')\n",
    "\n",
    "#check if the traces dir is empty and only then plot the middle sweep for each filename\n",
    "sort.plot_trace_if_not_done(work_dir, dir_plots, filenames)\n",
    "\n",
    "cortex_out_time = sort.get_datetime_from_input(active_chans_meta[0]['OP_time'][0])\n",
    "\n",
    "#creating the dataframe\n",
    "df_OP = pd.DataFrame(columns=['filename', 'slice', 'cell_ch', \n",
    "'hrs_after_slicing', 'Rs', 'Rin', 'resting_potential', 'max_spikes', \n",
    "'Rheobase', 'AP_heigth', 'TH', 'max_depol', 'max_repol', 'membra_time_constant_tau',\n",
    "'capacitance', 'AP_halfwidth','cell_type', 'inputs', 'AP_in_input', 'comment'])\n",
    "\n",
    "for i in range(len(indices_dict['vc'])):\n",
    "    vc = indices_dict['vc'][i]\n",
    "    vm = indices_dict['resting'][i]\n",
    "    char = indices_dict['characterization'][i]\n",
    "\n",
    "    slic = slice_names[vc]\n",
    "\n",
    "    filename_vc = work_dir + filenames[vc]\n",
    "    filename_vm = work_dir + filenames[vm]\n",
    "    filename_char = work_dir + filenames[char]\n",
    "\n",
    "    time_after_op = sort.get_time_after_OP(filename_char, cortex_out_time)\n",
    "\n",
    "    active_channels = active_chans_meta[0]['active_chans'][i]\n",
    "    \n",
    "    cell_IDs = hcf.get_cell_IDs(filename_char, slic, active_channels)\n",
    "    time_after_op = sort.get_time_after_OP(filename_char, cortex_out_time)\n",
    "    Rs, Rin = hcf.get_access_resistance(filename_vc, active_channels) \n",
    "    RMPs = hcf.get_RMP(filename_vm, active_channels)\n",
    "\n",
    "    params1_df = pd.DataFrame({'filename': filenames[char], 'slice' : slic, 'cell_ch': active_channels,\n",
    "    'hrs_after_slicing' : time_after_op, \n",
    "    'Rs' : Rs, 'Rin': Rin, 'resting_potential': RMPs})\n",
    "\n",
    "    inj = hcf.get_inj_current_steps(filename_char)\n",
    "    charact_params  = hcf.all_chracterization_params(filename_char, active_channels, inj, onset = 1000, offset = 11000)\n",
    "    df_char = pd.DataFrame.from_dict(charact_params)\n",
    "\n",
    "    df_to_add = pd.concat([params1_df, df_char], axis = 1)\n",
    "    df_OP = pd.concat([df_OP.loc[:], df_to_add]).reset_index(drop=True)\n",
    "\n",
    "    #plotting function\n",
    "    plotting_funcs.plot_vc_holding (filename_vc, active_channels)\n",
    "    plotting_funcs.plots_for_charact_file(filename_char, active_channels, inj)\n",
    "    \n",
    "OPs = pd.Series(OP).repeat(len(df_OP))\n",
    "researcher = pd.Series('Verji').repeat(len(df_OP))\n",
    "series_df = pd.DataFrame({'OP': OPs, 'patcher': researcher}).reset_index(drop=True)\n",
    "\n",
    "df_intrinsic = pd.concat([series_df, df_OP], axis = 1)\n",
    "\n",
    "df_intrinsic.to_excel(work_dir + 'data_tables/' + OP + '_Intrinsic_and_synaptic_properties.xlsx', index=False) \n",
    "#df_intrinsic.to_csv(work_dir + 'data_tables/' + OP[:-1] + '_Intrinsic_and_synaptic_properties.csv')\n",
    "\n",
    "print('Intrinsic properties DataFrame for  ' + OP + ' saved successfully. ' + '\\n' + 'Exclude recordings if necessary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ca9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connectivity screen analysis\n",
    "con_sccreen_connected_chans = active_chans_meta[1]\n",
    "\n",
    "for k, slic in enumerate(con_sccreen_connected_chans['con_screen_IC_file_indices']):\n",
    "    post_cells = con_sccreen_connected_chans['post_chans_VC'][k]\n",
    "    for cell in post_cells:\n",
    "        fn = work_dir + filenames[slic]\n",
    "        plotting_funcs.plot_trace(fn, 5, cell)\n",
    "\n",
    "\n",
    "#the following is a quick script to get a plot of the characterization and the input file\n",
    "\n",
    "# fn_indx = 22\n",
    "# active_channels = [1]\n",
    "#swp = 9\n",
    "channels = [1,2,4,7,8]\n",
    "indx_input_screen_fn = 32\n",
    "slic = 'S2_2'\n",
    "#plotting_funcs.plot_trace(work_dir + filenames[fn_indx], swp, chan)\n",
    "\n",
    "\n",
    "fn_input_screen = work_dir + filenames[indx_input_screen_fn]\n",
    "con_screen_data = hcf.load_traces(fn_input_screen)\n",
    "\n",
    "\n",
    "for chan in channels:\n",
    "    chan_name = 'Ch' + str(chan)\n",
    "    pre_sig = con_screen_data[chan_name][0]\n",
    "    vmO = np.mean(pre_sig, axis = 1) #from\n",
    "\n",
    "    fig,ax = plt.subplots(1,1, figsize = (5,3))\n",
    "    for i in range(np.shape(pre_sig)[1]):\n",
    "        ax.plot(pre_sig[10_000:17_000,i], alpha = 0.6, c = 'darkgrey', lw = 3.5)\n",
    "\n",
    "    ax.plot(vmO[10_000:17_000], c = 'green', alpha = 1, lw = 1.5, label = 'average input in TTX')\n",
    "    #ax.set_title(filenames[indx_input_screen_fn] + ' ' + chan_name)\n",
    "    #ax.set_axis_off()\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.set_yticks([-60, -40, -20, 0, 20, 40])\n",
    "    ax.set_ylabel('mV')\n",
    "    # fig.legend()\n",
    "    # plt.savefig('/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/results/mouse/ca1-sub-channorhodopsin_mice/Calb-CreSubiculum/inputs/legend_2' + '.jpg')\n",
    "    #plt.savefig('/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/results/mouse/ca1-sub-channorhodopsin_mice/Calb-CreSubiculum/inputs/TTX_4AP_fn_' + str(indx_input_screen_fn) + '_' +slic + 'Ch.' + str(chan) + '.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d6bc8b",
   "metadata": {},
   "source": [
    "## Analysis of averages from miniML output <a name = '5'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5aef3afa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'events_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m hrs_incub, cell_ID, recording_time, resting_potential, holding_minus_70_y_o_n, incubation_solution, \\\n\u001b[1;32m     21\u001b[0m     recording_in, tissue_source, patient_age, K_concentration \u001b[38;5;241m=\u001b[39m [], [], [], [], [], [], [], [], [], []\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#adding metadata columns \u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, fn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mevents_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecording filename\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     25\u001b[0m     chan \u001b[38;5;241m=\u001b[39m events_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mChannel\u001b[39m\u001b[38;5;124m'\u001b[39m][i]\n\u001b[1;32m     26\u001b[0m     dat_to_add \u001b[38;5;241m=\u001b[39m RMP_high_K_all[(RMP_high_K_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m fn) \u001b[38;5;241m&\u001b[39m \n\u001b[1;32m     27\u001b[0m                 (RMP_high_K_all[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcell_ch\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m chan)]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'events_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "verji_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/data_verji/'\n",
    "\n",
    "df_analyzed = pd.read_excel('/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/results/human/data/miniML/data_averages/all_analyzed_from_2023-11-21.xlsx')\n",
    "df_analyzed.drop(['Unnamed: 0'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "RMP_dirs = sorted(glob.glob(verji_dir + 'OP*' + '/data_tables/'  + '*RMP_high_K' + '*.xlsx'))\n",
    "\n",
    "RMP_high_K_all = pd.DataFrame()\n",
    "for df_path in RMP_dirs:\n",
    "    RMP_df = pd.read_excel(df_path)\n",
    "    RMP_high_K_all = pd.concat([RMP_high_K_all[:], RMP_df]).reset_index(drop=True)\n",
    "\n",
    "#not_included_OPs = list(set(list(RMP_high_K_all.OP.unique())) - set(list(events_df.OP.unique())))\n",
    "\n",
    "hrs_incub, cell_ID, recording_time, resting_potential, holding_minus_70_y_o_n, incubation_solution, \\\n",
    "    recording_in, tissue_source, patient_age, K_concentration = [], [], [], [], [], [], [], [], [], []\n",
    "\n",
    "#adding metadata columns \n",
    "for i, fn in enumerate(events_df['Recording filename']):\n",
    "    chan = events_df['Channel'][i]\n",
    "    dat_to_add = RMP_high_K_all[(RMP_high_K_all['filename'] == fn) & \n",
    "                (RMP_high_K_all['cell_ch'] == chan)]\n",
    "    if len(dat_to_add) == 0:\n",
    "        events_df =  events_df.drop(i)\n",
    "        continue\n",
    "\n",
    "    hrs_incub.append(dat_to_add['hrs_incubation'].item())\n",
    "    cell_ID.append(dat_to_add['cell_ID'].item())\n",
    "    recording_time.append(dat_to_add['recording_time'].item())\n",
    "    resting_potential.append(dat_to_add['resting_potential'].item()[0])\n",
    "    holding_minus_70_y_o_n.append(dat_to_add['holding_minus_70_y_o_n'].item())\n",
    "    incubation_solution.append(dat_to_add['incubation_solution'].item())\n",
    "    recording_in.append(dat_to_add['recording_in'].item())\n",
    "    tissue_source.append(dat_to_add['tissue_source'].item())\n",
    "    patient_age.append(dat_to_add['patient_age'].item())\n",
    "    K_concentration.append(dat_to_add['K concentration'].item())\n",
    "\n",
    "list_of_lists = [hrs_incub, cell_ID, recording_time, resting_potential, holding_minus_70_y_o_n, incubation_solution, \\\n",
    "    recording_in, tissue_source, patient_age, K_concentration]\n",
    "list_col_names = ['hrs_incub', 'cell_ID', 'recording_time', 'resting_potential', 'holding_minus_70_y_o_n', 'incubation_solution', \\\n",
    "    'recording_in', 'tissue_source', 'patient_age', 'K_concentration']\n",
    "\n",
    "#columns_to_include = list(set(list(RMP_high_K_all.columns)) - set(list(events_df.columns)))\n",
    "for i, col in enumerate(list_of_lists):\n",
    "    events_df.insert(i, list_col_names[i], col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dab849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting results\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "events_df_plot = events_df[(events_df['holding_minus_70_y_o_n'] == 'no') &\\\n",
    "    (events_df['K_concentration'] == 8) &\\\n",
    "         (events_df['recording_in'] != 'puff high K') &\\\n",
    "             (events_df['amplitude mean'] < 10)]\n",
    "\n",
    "# median_amp_no_hold_no_temp_highK = events_df['amplitude median'][(events_df['holding_minus_70_y_o_n'] == 'no') &\\\n",
    "#     (events_df['K_concentration'] == 8) &  (events_df['recording_in'] == 'high K')].values\n",
    "# median_amp_no_hold_no_temp_Ctrl = events_df['amplitude median'][(events_df['holding_minus_70_y_o_n'] == 'no') &\\\n",
    "#     (events_df['K_concentration'] == 8) &  (events_df['recording_in'] == 'Ctrl')].values\n",
    "# #remove APs\n",
    "# median_amp_no_hold_no_temp_Ctrl = median_amp_no_hold_no_temp_Ctrl[median_amp_no_hold_no_temp_Ctrl < 10] \n",
    "\n",
    "# freq_no_hold_no_temp_highK = events_df['frequency'][(events_df['holding_minus_70_y_o_n'] == 'no') &\\\n",
    "#     (events_df['K_concentration'] == 8) &  (events_df['recording_in'] == 'high K')].values\n",
    "# freq_no_hold_no_temp_Ctrl = events_df['frequency'][(events_df['holding_minus_70_y_o_n'] == 'no') &\\\n",
    "#     (events_df['K_concentration'] == 8) &  (events_df['recording_in'] == 'Ctrl')].values\n",
    "\n",
    "median_amp_no_hold_no_temp_highK = events_df_plot['amplitude median'][events_df['recording_in'] == 'high K'].values\n",
    "median_amp_no_hold_no_temp_Ctrl = events_df_plot['amplitude median'][events_df['recording_in'] == 'Ctrl'].values\n",
    "freq_no_hold_no_temp_highK = events_df_plot['frequency'][events_df['recording_in'] == 'high K'].values\n",
    "freq_no_hold_no_temp_Ctrl = events_df_plot['frequency'][events_df['recording_in'] == 'Ctrl'].values\n",
    "\n",
    "fig, ax = plt.subplots(2,1, sharex = False, figsize=(12,10))\n",
    "ax[0].scatter(np.linspace(0, 1,len(median_amp_no_hold_no_temp_Ctrl)), median_amp_no_hold_no_temp_Ctrl, alpha = 0.7, label = 'Ctrl')\n",
    "ax[0].scatter(np.linspace(2, 3,len(median_amp_no_hold_no_temp_highK)), median_amp_no_hold_no_temp_highK, alpha = 0.7, label = 'high K')\n",
    "ax[0].plot([0.4, 0.6], [np.nanmean(median_amp_no_hold_no_temp_Ctrl), np.nanmean(median_amp_no_hold_no_temp_Ctrl)], c = 'k')\n",
    "ax[0].plot([2.4, 2.6], [np.nanmean(median_amp_no_hold_no_temp_highK), np.nanmean(median_amp_no_hold_no_temp_highK)], c = 'k')\n",
    "\n",
    "ax[0].text(0.5, np.nanmean(median_amp_no_hold_no_temp_Ctrl) + 0.07,  str(round(np.nanmean(median_amp_no_hold_no_temp_Ctrl),2)), size = 15, c = 'k')\n",
    "ax[0].text(2.5, np.nanmean(median_amp_no_hold_no_temp_highK) + 0.07,  str(round(np.nanmean(median_amp_no_hold_no_temp_highK),2)), size = 15, c = 'k')\n",
    "ax[0].set_title('Median EPSP amplitude for cell per condition, no APs')\n",
    "\n",
    "ax[1].scatter(np.linspace(0, 1,len(freq_no_hold_no_temp_Ctrl)), freq_no_hold_no_temp_Ctrl, alpha = 0.7, label = 'Ctrl')\n",
    "ax[1].scatter(np.linspace(2, 3,len(freq_no_hold_no_temp_highK)), freq_no_hold_no_temp_highK, alpha = 0.7, label = 'high K')\n",
    "ax[1].plot([0.4, 0.6], [np.nanmean(freq_no_hold_no_temp_Ctrl), np.nanmean(freq_no_hold_no_temp_Ctrl)], c = 'k')\n",
    "ax[1].plot([2.4, 2.6], [np.nanmean(freq_no_hold_no_temp_highK), np.nanmean(freq_no_hold_no_temp_highK)], c = 'k')\n",
    "\n",
    "ax[1].text(0.5, np.nanmean(freq_no_hold_no_temp_Ctrl) + 0.1,  str(round(np.nanmean(freq_no_hold_no_temp_Ctrl),2)), size = 15, c = 'k')\n",
    "ax[1].text(2.5, np.nanmean(freq_no_hold_no_temp_highK) + 0.1,  str(round(np.nanmean(freq_no_hold_no_temp_highK),2)), size = 15, c = 'k')\n",
    "ax[1].set_title('Median EPSP frequency for cell per condition, no APs')\n",
    "\n",
    "ax[0].set_ylabel('Amplitude (mV)', fontsize = 15)\n",
    "ax[1].set_ylabel('Amplitude (mV)', fontsize = 15)\n",
    "\n",
    "ax[0].tick_params(axis='y', labelsize=15)\n",
    "ax[0].tick_params(axis='x', labelsize=15)\n",
    "ax[1].tick_params(axis='y', labelsize=15)\n",
    "ax[1].tick_params(axis='x', labelsize=15)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.patch.set_facecolor('white')\n",
    "fig.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#connect the lines -- FIX and FINALIZE\n",
    "for c, cell in enumerate(df_plot['cell_ID_new']):\n",
    "    #indx = index_s[c]\n",
    "    x_K = df_plot['x'].loc[df_plot['cell_ID_new'] == cell].tolist()[0]\n",
    "    x1 = [x_plot[0][c], x_K]\n",
    "    y = df[param][df['cell_ID_new'] == cell]\n",
    "    op = df_plot['OP'][df_plot['cell_ID_new'] == cell].tolist()[0]\n",
    "    plt.plot(x1, y, '-', color = op_color_dict[op], alpha = 0.5, linewidth = 2, zorder = 1)\n",
    "\n",
    "#seabornb plots to see trend\n",
    "\n",
    "sns.lineplot(x='recording_in', y='amplitude median', data=events_df_plot, palette='viridis', size='cell_ID', sizes=list(np.ones(24)+1),legend=False, errorbar=None)\n",
    "plt.show()\n",
    "sns.lineplot(x='recording_in', y='frequency', data=events_df_plot, palette='viridis', size='cell_ID', sizes=list(np.ones(24)+1),legend=False, errorbar=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e5a4c8",
   "metadata": {},
   "source": [
    "### Prepare files for analysis with Barbara's algorithm - manual analysis <a name ='6'>\n",
    "    \n",
    "Take all of the files which enter into the plots \n",
    "Take sweep 2 and 2nd to last and 20 seconds each - at the end of the sweep\n",
    "Run through Barbaraâ€™s algorithm and annotate the EPSPs manually with proof_read_results.py\n",
    "Compare to automatic detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de05729",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import sorting_functions as sort\n",
    "import shutil\n",
    "import os\n",
    "import human_characterisation_functions as hcf\n",
    "\n",
    "human_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/'\n",
    "EPSPs_meta_path = human_dir + 'meta_events/EPSPs/meta_dfs/2023-12-06EPSPs_meta.xlsx'\n",
    "meta_complete = pd.read_excel(EPSPs_meta_path)\n",
    "patcher = 'Verji'\n",
    "dest_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/meta_events/EPSPs_files/'\n",
    "\n",
    "for OP in meta_complete.OP.unique():\n",
    "    work_dir_events = sort.get_work_dir(human_dir, OP, patcher)\n",
    "    #moving files directly to the recordings folder in event analysis\n",
    "    for f in meta_complete['Name of recording'][meta_complete['OP'] == OP]:\n",
    "        shutil.copy(os.path.join(work_dir_events, f), dest_dir)\n",
    "\n",
    "swps_to_delete, cut_from_dp,swp_lens = [], [],[]\n",
    "file_list = sorted(os.listdir(dest_dir))\n",
    "for fn in meta_complete['Name of recording']:\n",
    "    data_dict = hcf.load_traces(dest_dir + fn)\n",
    "    ch0 = list(data_dict)[0]\n",
    "    sweep_len = np.shape(data_dict[ch0][0])[0]\n",
    "    sweep_num = np.shape(data_dict[ch0][0])[1] \n",
    "    swp_lens.append(sweep_len/20_000)\n",
    "\n",
    "    # if sweep_num > 5:\n",
    "    #     all_swps = np.arange(0,sweep_num-1)\n",
    "    #     keep_indx = [1, sweep_num-2]\n",
    "    #     swps_to_delete.append(np.delete(all_swps, keep_indx))\n",
    "    # else:\n",
    "    #     swps_to_delete.append([])\n",
    "    \n",
    "    # if sweep_len > 400_000:\n",
    "    #     cut_from_dp.append(400_000)\n",
    "    # else:\n",
    "    #     cut_from_dp.append(0)\n",
    "\n",
    "# meta_complete['swps_to_delete'] = swps_to_delete\n",
    "# meta_complete['Cut from datapoint'] = cut_from_dp\n",
    "#meta_complete.insert(6, 'sweep_len (ms)', swp_lens)\n",
    "\n",
    "# meta_complete.to_excel(human_dir + 'meta_events/EPSPs/meta_dfs/2023-12-06EPSPs_meta_for_Barbs_algorithm.xlsx')\n",
    "# meta_complete.to_excel('/Users/verjim/spontaneous-postsynaptic-currents-detection/metadata/metadata.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14833e76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501580cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
