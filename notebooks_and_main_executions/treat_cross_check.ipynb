{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import ephys_analysis.funcs_for_results_tables as get_results\n",
    "import ephys_analysis.funcs_plot_intrinsic_props as pl_intr\n",
    "import ephys_analysis.funcs_sorting as sort\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect dfs. Slice - treatment pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIR = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/results/human/data/treatment_cross_check/'\n",
    "\n",
    "# intrinsic df to OP241120\n",
    "df_intr_complete = pd.read_excel('/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/results/human/data/summary_data_tables/intrinsic_properties/2024-11-27_collected.xlsx')\n",
    "# df_intr = get_results.collect_intrinsic_df() # for most unpdated version\n",
    "\n",
    "# adding RMPs from charact\n",
    "# df_intr_complete = pl_intr.get_column_RMPs_from_char(df_intr)\n",
    "\n",
    "# only slice info \n",
    "df_intr = df_intr_complete[['OP', 'patcher', 'slice', 'treatment']]\n",
    "minis_df = get_results.collect_events_dfs('minis')[['OP', 'slice', 'patcher', 'treatment']]\n",
    "spontan_df = get_results.collect_events_dfs('spontan')[['OP', 'slice', 'patcher', 'treatment']]\n",
    "\n",
    "# merge all dfs\n",
    "collected_dfs = pd.concat([df_intr, minis_df, spontan_df], axis=0).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN_DIR = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/'\n",
    "PATHS = [HUMAN_DIR + 'data_verji/', HUMAN_DIR + 'data_rosie/']\n",
    "\n",
    "# collect from json meta_active_chans \n",
    "patchers_json, OPs_json, slices_json, treatments_json = [], [], [], []\n",
    "for dir in PATHS:\n",
    "    op_dirs = [d for d in os.listdir(dir) if d.startswith('OP')]\n",
    "    if 'rosie' in dir:\n",
    "        patcher = ['Rosie']\n",
    "    else:\n",
    "        patcher = ['Verji']\n",
    "    for op_dir in op_dirs:\n",
    "        # meta active chans\n",
    "        if op_dir == 'OP201020':\n",
    "            continue\n",
    "        meta_active_chans = glob.glob(f\"{dir + op_dir}/*meta_active_chans.json\")\n",
    "        if len(meta_active_chans) < 1:\n",
    "            # print('no meta_active_chans for ' + op_dir)\n",
    "            continue\n",
    "        elif len(meta_active_chans) >= 4:\n",
    "            active_chans_json = json.load(open(meta_active_chans[0]))\n",
    "            slices_json = slices_json + list(active_chans_json[3].keys())\n",
    "            treatments_json = treatments_json + list(active_chans_json[3].values())\n",
    "            patchers_json = patchers_json + patcher * len(active_chans_json[3].keys())\n",
    "            OPs_json = OPs_json + [op_dir] * len(active_chans_json[3].keys())\n",
    "            continue\n",
    "        elif len(meta_active_chans) < 4:\n",
    "            active_chans_json = json.load(open(meta_active_chans[0]))\n",
    "            slices_json = slices_json + active_chans_json[0]['slices']\n",
    "            treatments_json = treatments_json + active_chans_json[0]['treatment']\n",
    "            slices_json =  slices_json + active_chans_json[2]['mini_slices']\n",
    "            treatments_json = treatments_json + active_chans_json[2]['treatment']\n",
    "            OPs_json = OPs_json + [op_dir] * (len(active_chans_json[0]['slices']) \\\n",
    "                                                        + len(active_chans_json[2]['mini_slices']))\n",
    "            patchers_json = patchers_json + patcher * (len(active_chans_json[0]['slices']) \\\n",
    "                                                        + len(active_chans_json[2]['mini_slices']))\n",
    "\n",
    "# fixing the treatment, when list\n",
    "tr = []\n",
    "L = treatments_json\n",
    "for x in L:\n",
    "    if isinstance(x, list):\n",
    "        if len(x) < 1:\n",
    "            x = 'no treatment'\n",
    "        else:\n",
    "            x = x[0]\n",
    "    tr.append(x)\n",
    "treatments_json = tr\n",
    "\n",
    "# creating df from json meta_active_chans\n",
    "df_from_json = pd.DataFrame({'patcher': patchers_json, 'OP': OPs_json, 'slice': slices_json, 'treatment': treatments_json})\n",
    "df_from_json = df_from_json.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connectivity slices\n",
    "\n",
    "OPs_con, patchers_con, slices_con, treatments_con = [], [], [], []\n",
    "for dir in PATHS:\n",
    "    op_dirs = [d for d in os.listdir(dir) if d.startswith('OP')]\n",
    "    if 'rosie' in dir:\n",
    "        patcher = ['Rosie']\n",
    "    else:\n",
    "        patcher = ['Verji']\n",
    "    for op_dir in op_dirs:\n",
    "        con_screen_jsons = glob.glob(f\"{dir + op_dir}/*con_screen_only.json\")\n",
    "        if len(con_screen_jsons) < 1:\n",
    "            # print('no con_screen_json for ' + op_dir)\n",
    "            continue\n",
    "        else:\n",
    "            con_screen_json = json.load(open(con_screen_jsons[0]))[0]\n",
    "            OPs_con = OPs_con + [op_dir] * len(con_screen_json['slices'])\n",
    "            patchers_con = patchers_con + patcher * len(con_screen_json['slices'])\n",
    "            slices_con = slices_con + con_screen_json['slices']\n",
    "            treatments_con = treatments_con + con_screen_json['treatment']\n",
    "\n",
    "df_con_jsons = pd.DataFrame({'patcher': patchers_con, 'OP': OPs_con, 'slice': slices_con, \\\n",
    "                             'treatment': treatments_con}).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = pd.concat([collected_dfs, df_from_json, df_con_jsons], axis=0).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the slice treatment pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.sort_values(by=['patcher', 'OP', 'slice'])\n",
    "all_df.to_excel(SAVE_DIR + 'all_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reanalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/code/Human-slice-scripts/src/ephys_analysis/detect_peaks.py:11: UserWarning: A newest version is available at https://pypi.org/project/detecta/\n",
      "  warnings.warn('A newest version is available at https://pypi.org/project/detecta/')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import ephys_analysis.funcs_sorting as sort\n",
    "import ephys_analysis.funcs_for_results_tables as get_results\n",
    "import ephys_analysis.funcs_human_characterisation as hcf\n",
    "import json\n",
    "\n",
    "human_dir = '/Users/verjim/laptop_D_17.01.2022/Schmitz_lab/data/human/'\n",
    "\n",
    "OP = 'OP210319'\n",
    "patcher = 'Rosie'\n",
    "tissue_source = 'Virchow'\n",
    "age = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir, filenames, indices_dict, slice_names = sort.get_OP_metadata(human_dir, OP, patcher, 'old')\n",
    "\n",
    "# add correct vc_end to indices_dict\n",
    "if len(indices_dict['vc']) != len(indices_dict['vc_end']):\n",
    "    vc_ends = []\n",
    "    for vc in indices_dict['vc']:\n",
    "        vc_ends.append(input('vc_end_fn ' + str(vc))) # 000 when no vc_end\n",
    "\n",
    "    if len(indices_dict['vc']) == len(vc_ends):\n",
    "        indices_dict['vc_end'] = vc_ends\n",
    "\n",
    "    # save the fixed json\n",
    "    save_json_path = work_dir + OP + '_indices_dict.json'\n",
    "    with open(save_json_path, 'w') as file:\n",
    "        json.dump(indices_dict, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_chans_meta = sort.get_json_meta(human_dir, OP, patcher, '_meta_active_chans.json')[0]  \n",
    "cortex_out_time = sort.get_datetime_from_input(active_chans_meta['OP_time'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'filename' - characterizaiton 1 filename\n",
    "\n",
    "df_OP = pd.DataFrame(columns=['fn_vc', 'filename', 'slice', 'cell_ch', 'cell_ID', 'day', 'treatment', \n",
    "    'hrs_incubation', 'repatch', 'hrs_after_OP', 'Rs', 'Rin', 'resting_potential', 'max_spikes', \n",
    "    'Rheobase', 'AP_heigth', 'TH', 'max_depol', 'max_repol', 'membra_time_constant_tau', 'capacitance', \n",
    "    'Rheobse_ramp', 'AP_halfwidth', 'RMP_from_char'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, indx_vc in enumerate(indices_dict['vc']):\n",
    "    vc_end = indices_dict['vc_end'][i]\n",
    "    active_chans = active_chans_meta['active_chans'][i]\n",
    "    treatment = active_chans_meta['treatment'][i]\n",
    "    slice_name = active_chans_meta['slices'][i]\n",
    "    day = 'D1'\n",
    "    if slice_name[-2:] == 'D2':\n",
    "        day = 'D2'\n",
    "\n",
    "    fn_vc = work_dir + filenames[indx_vc]\n",
    "\n",
    "    time_after_op = sort.get_time_after_OP(fn_vc, cortex_out_time)\n",
    "    cell_IDs = hcf.get_cell_IDs(fn_vc, slice_name, active_chans)\n",
    "\n",
    "    Rs, Rin = hcf.get_access_resistance(fn_vc, active_chans)\n",
    "\n",
    "    params1_df = pd.DataFrame({'fn_vc': filenames[indx_vc], 'slice' : slice_name, \\\n",
    "                                   'cell_ch': active_chans,'cell_ID':cell_IDs, 'day' : day , \\\n",
    "                                    'treatment': treatment, 'Rs' : Rs, 'Rin': Rin, }).reset_index(drop=True)\n",
    "    \n",
    "    tissue = pd.Series(tissue_source).repeat(len(params1_df))\n",
    "    OPs = pd.Series(OP).repeat(len(params1_df))\n",
    "    researcher = pd.Series(patcher).repeat(len(params1_df))\n",
    "    patient_age = pd.Series(age).repeat(len(params1_df))\n",
    "    series_df = pd.DataFrame({'tissue_source': tissue, 'OP': OPs, 'patcher': researcher, 'patient_age': patient_age}).reset_index(drop=True)\n",
    "\n",
    "    df_vc = pd.concat([series_df, params1_df], axis = 1)\n",
    "    df_OP = pd.concat([df_OP.loc[:], df_vc], axis = 0)\n",
    "df_OP.to_excel(work_dir + 'data_tables/' + OP + '_Intrinsic_and_synaptic_properties_V1.xlsx', index=False) \n",
    "\n",
    "\n",
    "# get VC QC table\n",
    "get_results.get_QC_access_resistance_df (human_dir, OP, patcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build analysis pairs\n",
    "dict_vc_char, dict_vc_spontan = {}, {}\n",
    "\n",
    "for i, indx_vc in enumerate(indices_dict['vc']):\n",
    "    key = indx_vc\n",
    "    if i < len(indices_dict['vc']) - 1: # all but the last i\n",
    "        next_key = indices_dict['vc'][i + 1]\n",
    "        vals_char = [x for x in indices_dict['characterization'] if key < x < next_key]\n",
    "        vals_spontan = [x for x in indices_dict['spontan'] if key < x < next_key]\n",
    "    else: # last i\n",
    "        vals_char = [x for x in indices_dict['characterization'] if key < x]\n",
    "        vals_spontan = [x for x in indices_dict['spontan'] if key < x]\n",
    "    dict_vc_char[key] = vals_char\n",
    "    dict_vc_spontan[key] = vals_spontan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/f01m99kn5tx3chk04wp6sc440000gn/T/ipykernel_17424/4156013811.py:26: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_char_all = pd.concat([df_char_all.loc[:], df_char], axis = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n",
      "no 0mV step in the inj\n"
     ]
    }
   ],
   "source": [
    "df_char_all = pd.DataFrame(columns = ['max_spikes', 'Rheobase', 'AP_heigth', 'TH', 'max_depol', 'max_repol',\n",
    "       'membra_time_constant_tau', 'capacitance', 'AP_halfwidth'])\n",
    "\n",
    "fn_skip = []\n",
    "\n",
    "for i, indx_vc in enumerate(dict_vc_char.keys()):\n",
    "\n",
    "    for char_file in dict_vc_char[indx_vc]:\n",
    "\n",
    "        fn_char = work_dir + filenames[char_file]\n",
    "        active_channels = active_chans_meta['active_chans'][i]\n",
    "        inj = hcf.get_inj_current_steps(fn_char)\n",
    "\n",
    "        charact_params  = hcf.all_chracterization_params(fn_char, active_channels, inj)\n",
    "        df_char = pd.DataFrame.from_dict(charact_params)\n",
    "        RMPs = hcf.get_RMP_char_file(fn_char, active_channels)\n",
    "        time_after_op = sort.get_time_after_OP(fn_char, cortex_out_time)\n",
    "\n",
    "        df_char.insert(0, 'fn_char', filenames[char_file])\n",
    "        df_char.insert(1, 'fn_char_num', filenames[char_file])\n",
    "        df_char.insert(2, 'chans', active_channels)\n",
    "        df_char.insert(3, 'inj', str([float(x) for x in inj]))\n",
    "        df_char.insert(4, 'R MPs', RMPs)\n",
    "        df_char.insert(5, 'hrs_after_OP', time_after_op)\n",
    "\n",
    "        df_char_all = pd.concat([df_char_all.loc[:], df_char], axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-40.0, -80.0, -120.0, -160.0, -200.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ephys_analysis.funcs_plotting_raw_traces as pl_raw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
